{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to investigate how this method performs on models with and without alpha. My initial thoughts are that the model with alpha the predictions on average will be better so the scores are lower so the distances are smaller. \n",
    "\n",
    "How does this relate to distribution shift? We are interested in the time series nature of the data and how it deals with it so this still may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a TS, where the distribution changes at different points deppending on a provided list of distribution shifts.\n",
    "def Create_TimeSeries(s, dist_shifts):\n",
    "    n = s//len(dist_shifts)\n",
    "    m = s%len(dist_shifts)\n",
    "\n",
    "    final = np.array([])\n",
    "    \n",
    "    for i in range(len(dist_shifts)-1):\n",
    "        Y = abs(np.random.normal(dist_shifts[i][0], dist_shifts[i][1], n))\n",
    "        final = np.concatenate((final, Y))\n",
    "    \n",
    "    final = np.concatenate((final, abs(np.random.normal(dist_shifts[-1][0], dist_shifts[-1][1], n+m))))\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will just look at a simple random walk.\n",
    "\n",
    "p = 0.5\n",
    "pdist = [1-p, p]\n",
    "steps = [-1, 1]\n",
    "\n",
    "datapoints = 5\n",
    "seq_length = 100\n",
    "\n",
    "output_label_tuples = []\n",
    "\n",
    "dist_shift = [(0, 1), (-5, 1), (0, 10), (0.1, 0.1)]\n",
    "#dist_shift = [((-1**x) * 100, 100) if x%5==0 else (0, 0) for x in range(30)]\n",
    "\n",
    "for _ in range(datapoints):\n",
    "    X =  np.random.choice(steps, size=seq_length, p=pdist)\n",
    "    Y = Create_TimeSeries(seq_length, dist_shift)\n",
    "    T = np.cumsum(X*Y)\n",
    "    \n",
    "    input_data = T[:-1]\n",
    "    labels_data = T[1:]\n",
    "\n",
    "    output_label_tuples.append((input_data, labels_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Output = output_label_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(x):\n",
    "    add = 0.00\n",
    "    return x #+ (np.random.choice(steps, len(x), p=[0.5-add, 0.5+add]) * abs(np.random.normal(0, 1, len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would It not just be better to take the previous value as the thing -> this woudl make more sense but the whole of this technique is that it that should on any model. This is a good test for your models which you have been making as this is a sense where it is impossible to get any on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_window_size = 50\n",
    "alpha = 0.3\n",
    "orignal_alpha = alpha \n",
    "\n",
    "# This function predicts a range of Y_t at the alpha level. By using alpha_t.\n",
    "def C_t(alpha_t, scores, sigma_t, t):\n",
    "    Q = np.quantile(scores[:t], alpha_t)\n",
    "    positve_v = (sigma_t) + (abs(sigma_t) * Q)\n",
    "    negative_v = (sigma_t) - (abs(sigma_t) * Q)\n",
    "    return negative_v, positve_v\n",
    "\n",
    "# This function returns 1 if the prediction lies in the interval, 0 otherwise.\n",
    "def err_t(Y_t, C_t):\n",
    "    if C_t[0] < Y_t < C_t[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def l(B, theta):\n",
    "    return (orignal_alpha * (B - theta)) - min(0, (B - theta))\n",
    "    \n",
    "\n",
    "# We have 240 elememts we want a decent approximation of the quantile. So we will start at.\n",
    "\n",
    "\n",
    "\n",
    "total_coverage_list = []\n",
    "\n",
    "\n",
    "# The hyperparameters for the DtACI model.\n",
    "sigma = 0.1\n",
    "nu = 0.1\n",
    "\n",
    "e = 2.71 #np.e\n",
    "\n",
    "for m, (x, y) in enumerate(train_Output):\n",
    "    break\n",
    "    # Initialising the gamma candidates and the weights.\n",
    "    candiate_gamma = [0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "    candiate_alpha = [[0.1, 0.2, 0.3, 0.4, 0.5]]\n",
    "    gamma_weights = [[1.0 for _ in candiate_gamma]]\n",
    "    \n",
    "    error_list = []\n",
    "    Coverage_list = []\n",
    "    alpha_list = []\n",
    "    \n",
    "    alpha_error_list = []\n",
    "    \n",
    "    xpred = Model(x)\n",
    "    # Calculating the scores at each time step\n",
    "    All_scores = (abs(y - xpred))/abs(xpred)\n",
    "\n",
    "\n",
    "    # New thing to keep track of.\n",
    "    \n",
    "    for i in range(20, len(All_scores)):\n",
    "        # The probability of each gamma from the weights from stept.\n",
    "\n",
    "        Wt = sum(gamma_weights[-1])\n",
    "        gamma_probabilites = [w/Wt for w in gamma_weights[-1]]\n",
    "        \n",
    "        \n",
    "        # Choosing a alpha from the probabilites from the gamma candidates. Then calculating the coverage.\n",
    "        chosen_alpha_t = np.random.choice(candiate_alpha[-1], p=gamma_probabilites)\n",
    "        alpha_list.append(chosen_alpha_t)\n",
    "        Coverage_t = C_t(chosen_alpha_t, All_scores, xpred[i], i)\n",
    "        Coverage_list.append(Coverage_t)\n",
    "\n",
    "        # Updating the weights.\n",
    "        temp = [gamma_weights[j] * np.exp(-nu * l(y[i], candiate_alpha[-1][j])) for j in range(len(candiate_gamma))]\n",
    "        sumW, lenW = sum(temp), len(temp)\n",
    "        gamma_weights.append([(w*(1-sigma)) + sumW*(sigma/lenW) for w in temp])\n",
    "\n",
    "        # Calculating the coverage and error at each time step, for different alpha values.\n",
    "        alphai_errors = [err_t(y[i], C_t(alpha_i, All_scores, xpred[i], i)) for alpha_i in candiate_alpha[-1]]\n",
    "        alpha_error_list.append(alphai_errors)\n",
    "\n",
    "        err_true = err_t(y[i], C_t(chosen_alpha_t, All_scores, xpred[i], i))\n",
    "        error_list.append(err_true)\n",
    "\n",
    "        # Updating the alpha values.\n",
    "        candiate_alpha.append([alpha_i + (gamma_c * (orignal_alpha - alpha_i_err)) for alpha_i, alpha_i_err, gamma_c in zip(candiate_alpha[-1], alphai_errors, candiate_gamma)])\n",
    "        \n",
    "        # Mustgo back and check what beta is.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    coverage = 1 - pd.Series(error_list).mean()\n",
    "    total_coverage_list.append(coverage)\n",
    "    \n",
    "    if m<10:\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(30, 10))\n",
    "        \n",
    "        print(error_list)\n",
    "        axs[0][0].plot(1 - pd.Series(error_list).rolling(50).mean())\n",
    "        axs[0][0].axhline(coverage, color='r', linestyle='--')\n",
    "        axs[0][0].set_title('Coverage')\n",
    "        \n",
    "        axs[0][1].plot([ele[0] for ele in Coverage_list], label='Lower')\n",
    "        axs[0][1].plot([ele[1] for ele in Coverage_list], label='Upper')\n",
    "        axs[0][1].plot(y[20:])\n",
    "        axs[0][1].set_title('Prediction')\n",
    "        axs[0][1].legend()\n",
    "\n",
    "        axs[1][0].plot([ele[1]-ele[0] for ele in Coverage_list], label='Distance')\n",
    "        axs[1][0].axhline(np.mean([ele[1]-ele[0] for ele in Coverage_list]), color='r', linestyle='--')\n",
    "        axs[1][0].legend()\n",
    "        axs[1][0].set_title('Distance between upper and lower bounds')\n",
    "\n",
    "        axs[1][1].plot(alpha_list)\n",
    "        axs[1][1].set_title('Alpha')\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#print('The average coverage is', np.mean(total_coverage_list))\n",
    "\n",
    "# Would be useful to know the average distance between the upper and lower predictions.\n",
    "#average_distance = np.mean([ele[1] - ele[0] for ele in Coverage_list])\n",
    "#print('The average distance between the upper and lower predictions is', average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m sumW, lenW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(new_weights), \u001b[38;5;28mlen\u001b[39m(new_weights)\n\u001b[0;32m     82\u001b[0m final_weights \u001b[38;5;241m=\u001b[39m new_weights\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msigma) \u001b[38;5;241m+\u001b[39m sumW\u001b[38;5;241m*\u001b[39m(sigma\u001b[38;5;241m/\u001b[39mlenW)\n\u001b[1;32m---> 83\u001b[0m gamma_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Calculating the coverage and error at each time step, for different alpha values.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m alphai_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([err_t(y[i], C_t(alpha_i, All_scores, xpred[i], i)) \u001b[38;5;28;01mfor\u001b[39;00m alpha_i \u001b[38;5;129;01min\u001b[39;00m candiate_alpha[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numpy\\core\\shape_base.py:215\u001b[0m, in \u001b[0;36m_vhstack_dispatcher\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrays to stack must be passed as a \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    210\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuch as list or tuple.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(arrays)\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_vhstack_dispatcher\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _arrays_for_stack_dispatcher(tup)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_vhstack_dispatcher)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvstack\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inital_window_size = 50\n",
    "alpha = 0.3\n",
    "orignal_alpha = alpha \n",
    "\n",
    "# This function predicts a range of Y_t at the alpha level. By using alpha_t.\n",
    "def C_t(alpha_t, scores, sigma_t, t):\n",
    "    alpha_t = min(1, max(0, alpha_t))\n",
    "    Q = np.quantile(scores[:t], alpha_t)\n",
    "    positve_v = (sigma_t) + (abs(sigma_t) * Q)\n",
    "    negative_v = (sigma_t) - (abs(sigma_t) * Q)\n",
    "    return negative_v, positve_v\n",
    "\n",
    "# This function returns 1 if the prediction lies in the interval, 0 otherwise.\n",
    "def err_t(Y_t, C_t):\n",
    "    if C_t[0] < Y_t < C_t[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def l(B, theta):\n",
    "    return (orignal_alpha * (B - theta)) - min(0, (B - theta))\n",
    "\n",
    "\n",
    "\n",
    "l_vec = np.vectorize(l)\n",
    "    \n",
    "\n",
    "# We have 240 elememts we want a decent approximation of the quantile. So we will start at.\n",
    "\n",
    "\n",
    "\n",
    "total_coverage_list = []\n",
    "\n",
    "\n",
    "# The hyperparameters for the DtACI model.\n",
    "sigma = 0.1\n",
    "nu = 0.1\n",
    "\n",
    "\n",
    "\n",
    "for m, (x, y) in enumerate(train_Output[0:5]):\n",
    "    # Initialising the gamma candidates and the weights.\n",
    "    candiate_gamma = np.array([0.01, 0.02, 0.03, 0.05, 0.1])\n",
    "    candiate_alpha = np.array([[0.1, 0.2, 0.3, 0.4, 0.5]])\n",
    "    gamma_weights = np.array([[1.0 for _ in candiate_gamma]])\n",
    "    \n",
    "    error_list = []\n",
    "    Coverage_list = []\n",
    "    alpha_list = []\n",
    "    \n",
    "    alpha_error_list = []\n",
    "    \n",
    "    xpred = Model(x)\n",
    "    # Calculating the scores at each time step\n",
    "    All_scores = (abs(y - xpred))/abs(xpred)\n",
    "\n",
    "\n",
    "    # New thing to keep track of.\n",
    "    \n",
    "    for i in range(20, len(All_scores)):\n",
    "        # The probability of each gamma from the weights from stept.\n",
    "        Wt = sum(gamma_weights[-1])\n",
    "        gamma_probabilites = gamma_weights[-1]/Wt\n",
    "        \n",
    "        # Choosing a alpha from the probabilites from the gamma candidates. Then calculating the coverage.\n",
    "        chosen_alpha_t = np.random.choice(candiate_alpha[-1], p=gamma_probabilites)\n",
    "        alpha_list.append(chosen_alpha_t)\n",
    "        Coverage_t = C_t(chosen_alpha_t, All_scores, xpred[i], i)\n",
    "        Coverage_list.append(Coverage_t)\n",
    "\n",
    "        # We need to calculate B_t, which is the smallest value such that the obseved value is within the interval.\n",
    "        for possi in np.linspace(0, 1, 1000):\n",
    "            Cpossi= C_t(possi, All_scores, xpred[i], i)\n",
    "            if Cpossi[0] < y[i] < Cpossi[1]:\n",
    "                B_t = possi\n",
    "                break\n",
    "\n",
    "        \n",
    "        # Updating the weights.\n",
    "        new_weights = gamma_weights * np.exp(-nu * l_vec(B_t, candiate_alpha[-1]))\n",
    "        sumW, lenW = sum(new_weights), len(new_weights)\n",
    "        final_weights = new_weights*(1-sigma) + sumW*(sigma/lenW)\n",
    "        gamma_weights = np.vstack((gamma_weights, final_weights))\n",
    "\n",
    "        # Calculating the coverage and error at each time step, for different alpha values.\n",
    "        alphai_errors = np.array([err_t(y[i], C_t(alpha_i, All_scores, xpred[i], i)) for alpha_i in candiate_alpha[-1]])\n",
    "        alpha_error_list.append(alphai_errors)\n",
    "\n",
    "        err_true = err_t(y[i], C_t(chosen_alpha_t, All_scores, xpred[i], i))\n",
    "        error_list.append(err_true)\n",
    "\n",
    "        # Updating the alpha values.\n",
    "        new_alphas = candiate_alpha[-1] + (candiate_gamma * (orignal_alpha - alphai_errors))\n",
    "        candiate_alpha = np.vstack((candiate_alpha, new_alphas))\n",
    "\n",
    "        #plt.plot(alpha_list)\n",
    "        #plt.show()\n",
    "        \n",
    "        # Mustgo back and check what beta is.\n",
    "    \n",
    "    coverage = 1 - pd.Series(error_list).mean()\n",
    "    total_coverage_list.append(coverage)\n",
    "    \n",
    "    if m<10:\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(30, 10))\n",
    "        \n",
    "        print(error_list)\n",
    "        axs[0][0].plot(1 - pd.Series(error_list).rolling(5).mean())\n",
    "        axs[0][0].axhline(coverage, color='r', linestyle='--')\n",
    "        axs[0][0].set_title('Coverage')\n",
    "        \n",
    "        axs[0][1].plot([ele[0] for ele in Coverage_list], label='Lower')\n",
    "        axs[0][1].plot([ele[1] for ele in Coverage_list], label='Upper')\n",
    "        axs[0][1].plot(y[20:])\n",
    "        axs[0][1].set_title('Prediction')\n",
    "        axs[0][1].legend()\n",
    "\n",
    "        axs[1][0].plot([ele[1]-ele[0] for ele in Coverage_list], label='Distance')\n",
    "        axs[1][0].axhline(np.mean([ele[1]-ele[0] for ele in Coverage_list]), color='r', linestyle='--')\n",
    "        axs[1][0].legend()\n",
    "        axs[1][0].set_title('Distance between upper and lower bounds')\n",
    "\n",
    "        axs[1][1].plot(alpha_list)\n",
    "        axs[1][1].set_title('Alpha')\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('The average coverage is', np.mean(total_coverage_list))\n",
    "\n",
    "# Would be useful to know the average distance between the upper and lower predictions.\n",
    "average_distance = np.mean([ele[1] - ele[0] for ele in Coverage_list])\n",
    "print('The average distance between the upper and lower predictions is', average_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTF_Linear",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
