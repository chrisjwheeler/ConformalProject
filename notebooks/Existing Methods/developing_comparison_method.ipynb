{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best way to do this:\n",
    "#class which you load the data and then run compare which is a method, this will allow for easier loading. do this later.\n",
    "from ConformalMethods import ACP_data\n",
    "\n",
    "class Comparison:\n",
    "    # It might be easier to just covert eveything to var as we essentialy remove one datapoint.\n",
    "\n",
    "    def __init__(self, method_instance_dict):\n",
    "        '''Give methods and instances which need to be compared.'''\n",
    "        #takes dict then of the instances methods and args.\n",
    "        \n",
    "        self.method_dict = {'instance':,\n",
    "                       'methods':[],\n",
    "                       'requires_var': False,\n",
    "                       'args:': *args}\n",
    "        \n",
    "        self.run_dictionary = {}\n",
    "    \n",
    "        self.raw_data = None\n",
    "        self.corrected_dataset = None\n",
    "        self.xvy_dataset = None\n",
    "\n",
    "\n",
    "    def load_data(self, dataset: list):\n",
    "        '''Load the data that the comparison will run on. Give the data should be able to deal with var and non var data.'''\n",
    "        \n",
    "        self.raw_data = dataset\n",
    "        self.xvy_dataset = ACP_data.xvy_from_ACP(dataset)\n",
    "        self.corrected_dataset = ACP_data.xvy_correction(dataset)\n",
    "\n",
    "    def compute(self):\n",
    "        for dict in self.method_dict:\n",
    "            instance = dict['instance']\n",
    "            for method in dict['methods']:\n",
    "                for data in dataset:\n",
    "\n",
    "    def logic():\n",
    "        '''This is the logic that will be used for the comparison it will be run on every method and instance.'''\n",
    "        NotImplemented\n",
    "\n",
    "    def custom_logic():\n",
    "        '''This can be overwritten which will make a comparison easier'''\n",
    "        NotImplemented\n",
    "    \n",
    "    def Comparison():\n",
    "        '''This will run the comparison over all of the methods.'''\n",
    "        NotImplemented\n",
    "\n",
    "        # We should already have the raw comparisons and then shoudl loop throught the results.\n",
    "    \n",
    "    def save():\n",
    "        '''Save the results.'''\n",
    "        NotImplemented\n",
    "\n",
    "    def __str__():\n",
    "        '''This will show the comparison.'''\n",
    "        NotImplemented\n",
    "\n",
    "    def dashboard():\n",
    "        '''This will give a visualisation of the results.'''\n",
    "        NotImplemented\n",
    "\n",
    "# My thought now are that this might be too powerful for jupyter. I guess you can add saving logic to it as well. \n",
    "# This will be a large undertaking but as your current biggest weakness is that you are stuggling to compare results I think that it will be worth the effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_comparison(result_list_1, result_list_2):\n",
    "    relative_width = []\n",
    "    average_differnce_coverage = []\n",
    "    \n",
    "    for r1, r2 in zip(result_list_1, result_list_2):\n",
    "        relative_width.append(r1['average_prediction_interval'] / r2['average_prediction_interval'])\n",
    "        average_differnce_coverage.append(r1['realised_interval_coverage'] - r2['realised_interval_coverage'])\n",
    "\n",
    "    print(np.mean(relative_width))\n",
    "    print(np.mean(average_differnce_coverage))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
